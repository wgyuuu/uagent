# UAgent数据持久化接口设计

## 1. 概述

数据持久化接口是UAgent系统的基础设施层核心组件，负责工作流状态、上下文数据、执行历史等关键信息的持久化存储。该设计采用接口抽象模式，支持多种数据库后端的灵活切换。

## 2. 设计理念

### 2.1 接口抽象原则
- **数据库无关性**: 通过抽象接口屏蔽具体数据库实现
- **插件化架构**: 支持多种数据库后端的动态切换
- **一致性保证**: 提供事务支持和数据一致性保障
- **高性能设计**: 优化查询和批量操作性能

### 2.2 数据分层策略
- **热数据**: 活跃工作流和实时状态（Redis/内存）
- **温数据**: 近期历史和分析数据（PostgreSQL/MySQL）
- **冷数据**: 长期归档和审计日志（对象存储/文件系统）

### 2.3 扩展性考虑
- **水平扩展**: 支持分片和分布式部署
- **读写分离**: 支持主从复制和读写分离
- **缓存策略**: 多级缓存提升访问性能

## 3. 持久化接口架构

```
┌─────────────────────────────────────────────────────────────┐
│              Persistence Interface Layer                   │
├─────────────────────────────────────────────────────────────┤
│  Core Interfaces (核心接口)                                │
│  ├── WorkflowPersistence (工作流持久化)                    │
│  ├── ContextPersistence (上下文持久化)                     │
│  ├── ExecutionPersistence (执行历史持久化)                 │
│  └── ConfigurationPersistence (配置持久化)                 │
├─────────────────────────────────────────────────────────────┤
│  Abstract Base Classes (抽象基类)                          │
│  ├── BasePersistenceProvider (基础持久化提供者)            │
│  ├── TransactionManager (事务管理器)                       │
│  └── QueryBuilder (查询构建器)                             │
├─────────────────────────────────────────────────────────────┤
│  Implementation Layer (实现层)                             │
│  ├── PostgreSQLPersistence (PostgreSQL实现)               │
│  ├── MySQLPersistence (MySQL实现)                          │
│  ├── RedisPersistence (Redis实现)                          │
│  └── FilePersistence (文件系统实现)                        │
├─────────────────────────────────────────────────────────────┤
│  Optimization Layer (优化层)                               │
│  ├── ConnectionPool (连接池)                               │
│  ├── QueryOptimizer (查询优化器)                           │
│  ├── CacheManager (缓存管理器)                             │
│  └── PerformanceMonitor (性能监控器)                       │
└─────────────────────────────────────────────────────────────┘
```

## 4. 核心接口定义

### 4.1 工作流持久化接口
```python
from abc import ABC, abstractmethod
from typing import Dict, List, Optional, Any
from datetime import datetime
from dataclasses import dataclass

@dataclass
class WorkflowRecord:
    """工作流记录"""
    workflow_id: str
    execution_id: str
    name: str
    description: str
    status: str
    current_role_index: int
    roles: List[str]
    role_statuses: Dict[str, str]
    role_results: Dict[str, Any]
    created_at: datetime
    started_at: Optional[datetime] = None
    completed_at: Optional[datetime] = None
    created_by: str = ""
    metadata: Dict[str, Any] = None
    error_info: Optional[str] = None

class WorkflowPersistence(ABC):
    """工作流持久化接口"""
    
    @abstractmethod
    async def save_workflow(self, workflow: WorkflowRecord) -> bool:
        """保存工作流记录"""
        pass
    
    @abstractmethod
    async def update_workflow(self, workflow_id: str, updates: Dict[str, Any]) -> bool:
        """更新工作流记录"""
        pass
    
    @abstractmethod
    async def get_workflow(self, workflow_id: str) -> Optional[WorkflowRecord]:
        """获取工作流记录"""
        pass
    
    @abstractmethod
    async def list_workflows(self, 
                           status: Optional[str] = None,
                           created_by: Optional[str] = None,
                           limit: int = 100,
                           offset: int = 0) -> List[WorkflowRecord]:
        """列出工作流记录"""
        pass
    
    @abstractmethod
    async def delete_workflow(self, workflow_id: str) -> bool:
        """删除工作流记录"""
        pass
    
    @abstractmethod
    async def get_active_workflows(self) -> List[WorkflowRecord]:
        """获取活跃的工作流"""
        pass
    
    @abstractmethod
    async def update_workflow_status(self, 
                                   workflow_id: str, 
                                   status: str,
                                   error_info: Optional[str] = None) -> bool:
        """更新工作流状态"""
        pass
    
    @abstractmethod
    async def update_role_status(self, 
                               workflow_id: str,
                               role: str,
                               status: str,
                               result: Optional[Any] = None) -> bool:
        """更新角色状态"""
        pass
    
    @abstractmethod
    async def get_workflow_statistics(self, 
                                    start_date: Optional[datetime] = None,
                                    end_date: Optional[datetime] = None) -> Dict[str, Any]:
        """获取工作流统计信息"""
        pass
```

### 4.2 上下文持久化接口
```python
@dataclass
class ContextRecord:
    """上下文记录"""
    context_id: str
    workflow_id: str
    role: str
    context_type: str  # isolated, compressed, handoff
    sections: Dict[str, Any]
    created_at: datetime
    last_updated: datetime
    compressed_content: Optional[str] = None
    compression_ratio: Optional[float] = None
    metadata: Dict[str, Any] = None

class ContextPersistence(ABC):
    """上下文持久化接口"""
    
    @abstractmethod
    async def save_context(self, context: ContextRecord) -> bool:
        """保存上下文记录"""
        pass
    
    @abstractmethod
    async def update_context(self, context_id: str, updates: Dict[str, Any]) -> bool:
        """更新上下文记录"""
        pass
    
    @abstractmethod
    async def get_context(self, context_id: str) -> Optional[ContextRecord]:
        """获取上下文记录"""
        pass
    
    @abstractmethod
    async def list_contexts_by_workflow(self, workflow_id: str) -> List[ContextRecord]:
        """按工作流列出上下文"""
        pass
    
    @abstractmethod
    async def list_contexts_by_role(self, role: str, limit: int = 100) -> List[ContextRecord]:
        """按角色列出上下文"""
        pass
    
    @abstractmethod
    async def delete_context(self, context_id: str) -> bool:
        """删除上下文记录"""
        pass
    
    @abstractmethod
    async def cleanup_expired_contexts(self, 
                                     expiry_hours: int = 24) -> int:
        """清理过期上下文"""
        pass
    
    @abstractmethod
    async def save_compressed_context(self, 
                                    context_id: str,
                                    compressed_content: str,
                                    compression_ratio: float) -> bool:
        """保存压缩上下文"""
        pass
    
    @abstractmethod
    async def get_context_history(self, 
                                context_id: str,
                                limit: int = 10) -> List[Dict[str, Any]]:
        """获取上下文变更历史"""
        pass
```

### 4.3 执行历史持久化接口
```python
@dataclass
class ExecutionRecord:
    """执行记录"""
    execution_id: str
    workflow_id: str
    role: str
    tool_name: Optional[str]
    action_type: str  # role_execution, tool_call, handoff, error
    input_data: Dict[str, Any]
    output_data: Dict[str, Any]
    status: str  # success, failed, cancelled
    started_at: datetime
    completed_at: Optional[datetime] = None
    execution_time: Optional[float] = None
    error_message: Optional[str] = None
    metadata: Dict[str, Any] = None

class ExecutionPersistence(ABC):
    """执行历史持久化接口"""
    
    @abstractmethod
    async def save_execution(self, execution: ExecutionRecord) -> bool:
        """保存执行记录"""
        pass
    
    @abstractmethod
    async def update_execution(self, execution_id: str, updates: Dict[str, Any]) -> bool:
        """更新执行记录"""
        pass
    
    @abstractmethod
    async def get_execution(self, execution_id: str) -> Optional[ExecutionRecord]:
        """获取执行记录"""
        pass
    
    @abstractmethod
    async def list_executions_by_workflow(self, 
                                        workflow_id: str,
                                        limit: int = 100) -> List[ExecutionRecord]:
        """按工作流列出执行记录"""
        pass
    
    @abstractmethod
    async def list_executions_by_role(self, 
                                    role: str,
                                    status: Optional[str] = None,
                                    limit: int = 100) -> List[ExecutionRecord]:
        """按角色列出执行记录"""
        pass
    
    @abstractmethod
    async def get_execution_statistics(self, 
                                     role: Optional[str] = None,
                                     start_date: Optional[datetime] = None,
                                     end_date: Optional[datetime] = None) -> Dict[str, Any]:
        """获取执行统计信息"""
        pass
    
    @abstractmethod
    async def cleanup_old_executions(self, 
                                   retention_days: int = 30) -> int:
        """清理旧执行记录"""
        pass
```

### 4.4 配置持久化接口
```python
@dataclass
class ConfigurationRecord:
    """配置记录"""
    config_id: str
    config_type: str  # role_config, system_config, user_config
    name: str
    content: Dict[str, Any]
    version: str
    created_at: datetime
    updated_at: datetime
    created_by: str
    is_active: bool = True
    metadata: Dict[str, Any] = None

class ConfigurationPersistence(ABC):
    """配置持久化接口"""
    
    @abstractmethod
    async def save_configuration(self, config: ConfigurationRecord) -> bool:
        """保存配置记录"""
        pass
    
    @abstractmethod
    async def update_configuration(self, config_id: str, updates: Dict[str, Any]) -> bool:
        """更新配置记录"""
        pass
    
    @abstractmethod
    async def get_configuration(self, config_id: str) -> Optional[ConfigurationRecord]:
        """获取配置记录"""
        pass
    
    @abstractmethod
    async def list_configurations(self, 
                                config_type: Optional[str] = None,
                                is_active: bool = True) -> List[ConfigurationRecord]:
        """列出配置记录"""
        pass
    
    @abstractmethod
    async def delete_configuration(self, config_id: str) -> bool:
        """删除配置记录"""
        pass
    
    @abstractmethod
    async def get_active_configuration(self, 
                                     config_type: str,
                                     name: str) -> Optional[ConfigurationRecord]:
        """获取活跃配置"""
        pass
```

## 5. 抽象基类实现

### 5.1 基础持久化提供者
```python
class BasePersistenceProvider(ABC):
    """基础持久化提供者"""
    
    def __init__(self, connection_config: Dict[str, Any]):
        self.connection_config = connection_config
        self.connection_pool = None
        self.transaction_manager = None
        self.query_builder = None
        self.is_connected = False
    
    @abstractmethod
    async def connect(self) -> bool:
        """建立连接"""
        pass
    
    @abstractmethod
    async def disconnect(self) -> bool:
        """断开连接"""
        pass
    
    @abstractmethod
    async def health_check(self) -> bool:
        """健康检查"""
        pass
    
    @abstractmethod
    async def execute_query(self, query: str, parameters: Dict[str, Any] = None) -> Any:
        """执行查询"""
        pass
    
    @abstractmethod
    async def execute_transaction(self, operations: List[Dict[str, Any]]) -> bool:
        """执行事务"""
        pass
    
    async def initialize_schema(self) -> bool:
        """初始化数据库架构"""
        try:
            schema_queries = await self._get_schema_queries()
            for query in schema_queries:
                await self.execute_query(query)
            return True
        except Exception as e:
            logger.error(f"Schema initialization failed: {e}")
            return False
    
    @abstractmethod
    async def _get_schema_queries(self) -> List[str]:
        """获取架构创建查询"""
        pass
```

### 5.2 事务管理器
```python
from contextlib import asynccontextmanager

class TransactionManager:
    """事务管理器"""
    
    def __init__(self, persistence_provider: BasePersistenceProvider):
        self.provider = persistence_provider
        self.active_transactions: Dict[str, Any] = {}
    
    @asynccontextmanager
    async def transaction(self, transaction_id: Optional[str] = None):
        """事务上下文管理器"""
        if transaction_id is None:
            transaction_id = f"txn_{uuid4().hex[:8]}"
        
        try:
            # 开始事务
            await self._begin_transaction(transaction_id)
            yield transaction_id
            
            # 提交事务
            await self._commit_transaction(transaction_id)
            
        except Exception as e:
            # 回滚事务
            await self._rollback_transaction(transaction_id)
            raise e
        finally:
            # 清理事务资源
            await self._cleanup_transaction(transaction_id)
    
    async def _begin_transaction(self, transaction_id: str):
        """开始事务"""
        # 具体实现取决于数据库类型
        pass
    
    async def _commit_transaction(self, transaction_id: str):
        """提交事务"""
        # 具体实现取决于数据库类型
        pass
    
    async def _rollback_transaction(self, transaction_id: str):
        """回滚事务"""
        # 具体实现取决于数据库类型
        pass
    
    async def _cleanup_transaction(self, transaction_id: str):
        """清理事务资源"""
        if transaction_id in self.active_transactions:
            del self.active_transactions[transaction_id]
```

### 5.3 查询构建器
```python
class QueryBuilder:
    """查询构建器"""
    
    def __init__(self, dialect: str = "postgresql"):
        self.dialect = dialect
        self.query_templates = self._load_query_templates()
    
    def build_insert_query(self, 
                          table: str, 
                          data: Dict[str, Any],
                          on_conflict: str = "IGNORE") -> tuple:
        """构建插入查询"""
        columns = list(data.keys())
        placeholders = [f"${i+1}" for i in range(len(columns))]
        
        query = f"""
        INSERT INTO {table} ({', '.join(columns)})
        VALUES ({', '.join(placeholders)})
        """
        
        if on_conflict == "UPDATE":
            update_clause = ", ".join([f"{col} = EXCLUDED.{col}" for col in columns])
            query += f" ON CONFLICT DO UPDATE SET {update_clause}"
        elif on_conflict == "IGNORE":
            query += " ON CONFLICT DO NOTHING"
        
        return query, list(data.values())
    
    def build_update_query(self, 
                          table: str,
                          data: Dict[str, Any],
                          where_conditions: Dict[str, Any]) -> tuple:
        """构建更新查询"""
        set_clauses = [f"{col} = ${i+1}" for i, col in enumerate(data.keys())]
        where_clauses = [f"{col} = ${i+len(data)+1}" for i, col in enumerate(where_conditions.keys())]
        
        query = f"""
        UPDATE {table}
        SET {', '.join(set_clauses)}
        WHERE {' AND '.join(where_clauses)}
        """
        
        parameters = list(data.values()) + list(where_conditions.values())
        return query, parameters
    
    def build_select_query(self, 
                          table: str,
                          columns: List[str] = None,
                          where_conditions: Dict[str, Any] = None,
                          order_by: List[str] = None,
                          limit: Optional[int] = None,
                          offset: Optional[int] = None) -> tuple:
        """构建查询语句"""
        
        # 选择列
        if columns:
            select_clause = ", ".join(columns)
        else:
            select_clause = "*"
        
        query = f"SELECT {select_clause} FROM {table}"
        parameters = []
        
        # WHERE条件
        if where_conditions:
            where_clauses = []
            for i, (col, value) in enumerate(where_conditions.items()):
                where_clauses.append(f"{col} = ${len(parameters)+1}")
                parameters.append(value)
            query += f" WHERE {' AND '.join(where_clauses)}"
        
        # ORDER BY
        if order_by:
            query += f" ORDER BY {', '.join(order_by)}"
        
        # LIMIT和OFFSET
        if limit:
            query += f" LIMIT {limit}"
        if offset:
            query += f" OFFSET {offset}"
        
        return query, parameters
    
    def build_delete_query(self, 
                          table: str,
                          where_conditions: Dict[str, Any]) -> tuple:
        """构建删除查询"""
        where_clauses = [f"{col} = ${i+1}" for i, col in enumerate(where_conditions.keys())]
        
        query = f"""
        DELETE FROM {table}
        WHERE {' AND '.join(where_clauses)}
        """
        
        return query, list(where_conditions.values())
```

## 6. PostgreSQL实现示例

### 6.1 PostgreSQL持久化实现
```python
import asyncpg
from typing import Dict, List, Optional, Any

class PostgreSQLPersistence(BasePersistenceProvider, 
                           WorkflowPersistence, 
                           ContextPersistence, 
                           ExecutionPersistence,
                           ConfigurationPersistence):
    """PostgreSQL持久化实现"""
    
    def __init__(self, connection_config: Dict[str, Any]):
        super().__init__(connection_config)
        self.pool: Optional[asyncpg.Pool] = None
        self.query_builder = QueryBuilder("postgresql")
    
    async def connect(self) -> bool:
        """建立连接池"""
        try:
            self.pool = await asyncpg.create_pool(
                host=self.connection_config.get("host", "localhost"),
                port=self.connection_config.get("port", 5432),
                user=self.connection_config.get("user", "postgres"),
                password=self.connection_config.get("password", ""),
                database=self.connection_config.get("database", "uagent"),
                min_size=self.connection_config.get("min_connections", 5),
                max_size=self.connection_config.get("max_connections", 20)
            )
            
            self.is_connected = True
            await self.initialize_schema()
            return True
            
        except Exception as e:
            logger.error(f"PostgreSQL connection failed: {e}")
            return False
    
    async def disconnect(self) -> bool:
        """断开连接"""
        if self.pool:
            await self.pool.close()
            self.is_connected = False
        return True
    
    async def health_check(self) -> bool:
        """健康检查"""
        if not self.pool:
            return False
        
        try:
            async with self.pool.acquire() as conn:
                await conn.fetchval("SELECT 1")
            return True
        except Exception:
            return False
    
    async def execute_query(self, query: str, parameters: List[Any] = None) -> Any:
        """执行查询"""
        async with self.pool.acquire() as conn:
            if parameters:
                return await conn.fetch(query, *parameters)
            else:
                return await conn.fetch(query)
    
    # 工作流持久化实现
    async def save_workflow(self, workflow: WorkflowRecord) -> bool:
        """保存工作流记录"""
        try:
            query, params = self.query_builder.build_insert_query(
                "workflows",
                {
                    "workflow_id": workflow.workflow_id,
                    "execution_id": workflow.execution_id,
                    "name": workflow.name,
                    "description": workflow.description,
                    "status": workflow.status,
                    "current_role_index": workflow.current_role_index,
                    "roles": json.dumps(workflow.roles),
                    "role_statuses": json.dumps(workflow.role_statuses),
                    "role_results": json.dumps(workflow.role_results),
                    "created_at": workflow.created_at,
                    "started_at": workflow.started_at,
                    "completed_at": workflow.completed_at,
                    "created_by": workflow.created_by,
                    "metadata": json.dumps(workflow.metadata or {}),
                    "error_info": workflow.error_info
                },
                on_conflict="UPDATE"
            )
            
            await self.execute_query(query, params)
            return True
            
        except Exception as e:
            logger.error(f"Failed to save workflow: {e}")
            return False
    
    async def get_workflow(self, workflow_id: str) -> Optional[WorkflowRecord]:
        """获取工作流记录"""
        try:
            query, params = self.query_builder.build_select_query(
                "workflows",
                where_conditions={"workflow_id": workflow_id}
            )
            
            result = await self.execute_query(query, params)
            
            if result:
                row = result[0]
                return WorkflowRecord(
                    workflow_id=row["workflow_id"],
                    execution_id=row["execution_id"],
                    name=row["name"],
                    description=row["description"],
                    status=row["status"],
                    current_role_index=row["current_role_index"],
                    roles=json.loads(row["roles"]),
                    role_statuses=json.loads(row["role_statuses"]),
                    role_results=json.loads(row["role_results"]),
                    created_at=row["created_at"],
                    started_at=row["started_at"],
                    completed_at=row["completed_at"],
                    created_by=row["created_by"],
                    metadata=json.loads(row["metadata"]),
                    error_info=row["error_info"]
                )
            
            return None
            
        except Exception as e:
            logger.error(f"Failed to get workflow: {e}")
            return None
    
    async def _get_schema_queries(self) -> List[str]:
        """获取PostgreSQL架构创建查询"""
        return [
            """
            CREATE TABLE IF NOT EXISTS workflows (
                workflow_id VARCHAR(255) PRIMARY KEY,
                execution_id VARCHAR(255) NOT NULL,
                name VARCHAR(255) NOT NULL,
                description TEXT,
                status VARCHAR(50) NOT NULL,
                current_role_index INTEGER DEFAULT 0,
                roles JSONB NOT NULL,
                role_statuses JSONB DEFAULT '{}',
                role_results JSONB DEFAULT '{}',
                created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
                started_at TIMESTAMP WITH TIME ZONE,
                completed_at TIMESTAMP WITH TIME ZONE,
                created_by VARCHAR(255) DEFAULT '',
                metadata JSONB DEFAULT '{}',
                error_info TEXT
            )
            """,
            """
            CREATE INDEX IF NOT EXISTS idx_workflows_status ON workflows(status);
            """,
            """
            CREATE INDEX IF NOT EXISTS idx_workflows_created_by ON workflows(created_by);
            """,
            """
            CREATE TABLE IF NOT EXISTS contexts (
                context_id VARCHAR(255) PRIMARY KEY,
                workflow_id VARCHAR(255) NOT NULL,
                role VARCHAR(100) NOT NULL,
                context_type VARCHAR(50) NOT NULL,
                sections JSONB NOT NULL,
                created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
                last_updated TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
                compressed_content TEXT,
                compression_ratio FLOAT,
                metadata JSONB DEFAULT '{}'
            )
            """,
            """
            CREATE INDEX IF NOT EXISTS idx_contexts_workflow ON contexts(workflow_id);
            """,
            """
            CREATE INDEX IF NOT EXISTS idx_contexts_role ON contexts(role);
            """,
            """
            CREATE TABLE IF NOT EXISTS executions (
                execution_id VARCHAR(255) PRIMARY KEY,
                workflow_id VARCHAR(255) NOT NULL,
                role VARCHAR(100) NOT NULL,
                tool_name VARCHAR(255),
                action_type VARCHAR(50) NOT NULL,
                input_data JSONB DEFAULT '{}',
                output_data JSONB DEFAULT '{}',
                status VARCHAR(50) NOT NULL,
                started_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
                completed_at TIMESTAMP WITH TIME ZONE,
                execution_time FLOAT,
                error_message TEXT,
                metadata JSONB DEFAULT '{}'
            )
            """,
            """
            CREATE INDEX IF NOT EXISTS idx_executions_workflow ON executions(workflow_id);
            """,
            """
            CREATE INDEX IF NOT EXISTS idx_executions_role ON executions(role);
            """,
            """
            CREATE TABLE IF NOT EXISTS configurations (
                config_id VARCHAR(255) PRIMARY KEY,
                config_type VARCHAR(50) NOT NULL,
                name VARCHAR(255) NOT NULL,
                content JSONB NOT NULL,
                version VARCHAR(50) NOT NULL,
                created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
                updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
                created_by VARCHAR(255) NOT NULL,
                is_active BOOLEAN DEFAULT TRUE,
                metadata JSONB DEFAULT '{}'
            )
            """,
            """
            CREATE INDEX IF NOT EXISTS idx_configurations_type ON configurations(config_type);
            """,
            """
            CREATE INDEX IF NOT EXISTS idx_configurations_active ON configurations(is_active);
            """
        ]
```

## 7. 缓存策略实现

### 7.1 多级缓存管理器
```python
class CacheManager:
    """缓存管理器"""
    
    def __init__(self, redis_config: Dict[str, Any] = None):
        self.memory_cache = {}  # L1缓存：内存
        self.redis_client = None  # L2缓存：Redis
        self.cache_ttl = {
            "workflow": 300,    # 5分钟
            "context": 600,     # 10分钟
            "execution": 1800,  # 30分钟
            "config": 3600      # 1小时
        }
        
        if redis_config:
            self._init_redis_cache(redis_config)
    
    async def _init_redis_cache(self, redis_config: Dict[str, Any]):
        """初始化Redis缓存"""
        try:
            import aioredis
            self.redis_client = aioredis.from_url(
                f"redis://{redis_config.get('host', 'localhost')}:{redis_config.get('port', 6379)}",
                password=redis_config.get('password'),
                db=redis_config.get('db', 0)
            )
        except ImportError:
            logger.warning("aioredis not installed, Redis cache disabled")
    
    async def get(self, key: str, cache_type: str = "default") -> Optional[Any]:
        """获取缓存"""
        
        # L1缓存检查
        if key in self.memory_cache:
            cached_item = self.memory_cache[key]
            if cached_item["expires_at"] > datetime.now():
                return cached_item["value"]
            else:
                del self.memory_cache[key]
        
        # L2缓存检查
        if self.redis_client:
            try:
                cached_data = await self.redis_client.get(key)
                if cached_data:
                    value = json.loads(cached_data)
                    # 回填L1缓存
                    await self.set(key, value, cache_type, update_redis=False)
                    return value
            except Exception as e:
                logger.warning(f"Redis cache get failed: {e}")
        
        return None
    
    async def set(self, 
                 key: str, 
                 value: Any, 
                 cache_type: str = "default",
                 ttl: Optional[int] = None,
                 update_redis: bool = True) -> bool:
        """设置缓存"""
        
        if ttl is None:
            ttl = self.cache_ttl.get(cache_type, 300)
        
        expires_at = datetime.now() + timedelta(seconds=ttl)
        
        # 设置L1缓存
        self.memory_cache[key] = {
            "value": value,
            "expires_at": expires_at
        }
        
        # 设置L2缓存
        if self.redis_client and update_redis:
            try:
                serialized_value = json.dumps(value, default=str)
                await self.redis_client.setex(key, ttl, serialized_value)
            except Exception as e:
                logger.warning(f"Redis cache set failed: {e}")
        
        return True
    
    async def delete(self, key: str) -> bool:
        """删除缓存"""
        
        # 删除L1缓存
        if key in self.memory_cache:
            del self.memory_cache[key]
        
        # 删除L2缓存
        if self.redis_client:
            try:
                await self.redis_client.delete(key)
            except Exception as e:
                logger.warning(f"Redis cache delete failed: {e}")
        
        return True
    
    async def clear_cache_by_pattern(self, pattern: str) -> int:
        """按模式清理缓存"""
        cleared_count = 0
        
        # 清理L1缓存
        keys_to_remove = [key for key in self.memory_cache.keys() if pattern in key]
        for key in keys_to_remove:
            del self.memory_cache[key]
            cleared_count += 1
        
        # 清理L2缓存
        if self.redis_client:
            try:
                keys = await self.redis_client.keys(f"*{pattern}*")
                if keys:
                    await self.redis_client.delete(*keys)
                    cleared_count += len(keys)
            except Exception as e:
                logger.warning(f"Redis pattern delete failed: {e}")
        
        return cleared_count
```

### 7.2 缓存装饰器
```python
def cached(cache_type: str = "default", ttl: Optional[int] = None):
    """缓存装饰器"""
    def decorator(func):
        @functools.wraps(func)
        async def wrapper(self, *args, **kwargs):
            # 生成缓存键
            cache_key = f"{func.__name__}:{hash(str(args) + str(sorted(kwargs.items())))}"
            
            # 尝试从缓存获取
            if hasattr(self, 'cache_manager'):
                cached_result = await self.cache_manager.get(cache_key, cache_type)
                if cached_result is not None:
                    return cached_result
            
            # 执行原函数
            result = await func(self, *args, **kwargs)
            
            # 缓存结果
            if hasattr(self, 'cache_manager') and result is not None:
                await self.cache_manager.set(cache_key, result, cache_type, ttl)
            
            return result
        return wrapper
    return decorator
```

## 8. 性能优化

### 8.1 连接池优化
```python
class OptimizedConnectionPool:
    """优化的连接池"""
    
    def __init__(self, 
                 connection_factory,
                 min_size: int = 5,
                 max_size: int = 20,
                 max_idle_time: int = 300):
        self.connection_factory = connection_factory
        self.min_size = min_size
        self.max_size = max_size
        self.max_idle_time = max_idle_time
        
        self.available_connections = asyncio.Queue()
        self.active_connections = set()
        self.connection_stats = {}
        self.pool_lock = asyncio.Lock()
    
    async def get_connection(self):
        """获取连接"""
        async with self.pool_lock:
            # 尝试从可用连接池获取
            if not self.available_connections.empty():
                conn = await self.available_connections.get()
                
                # 检查连接健康状态
                if await self._is_connection_healthy(conn):
                    self.active_connections.add(conn)
                    return conn
                else:
                    await self._close_connection(conn)
            
            # 创建新连接
            if len(self.active_connections) < self.max_size:
                conn = await self._create_connection()
                self.active_connections.add(conn)
                return conn
            
            # 等待连接释放
            return await self.available_connections.get()
    
    async def return_connection(self, conn):
        """归还连接"""
        async with self.pool_lock:
            if conn in self.active_connections:
                self.active_connections.remove(conn)
                
                if await self._is_connection_healthy(conn):
                    await self.available_connections.put(conn)
                else:
                    await self._close_connection(conn)
    
    async def _create_connection(self):
        """创建新连接"""
        conn = await self.connection_factory()
        self.connection_stats[id(conn)] = {
            "created_at": datetime.now(),
            "last_used": datetime.now(),
            "use_count": 0
        }
        return conn
    
    async def _is_connection_healthy(self, conn) -> bool:
        """检查连接健康状态"""
        try:
            # 具体健康检查逻辑
            await conn.fetchval("SELECT 1")
            
            # 更新使用统计
            conn_id = id(conn)
            if conn_id in self.connection_stats:
                self.connection_stats[conn_id]["last_used"] = datetime.now()
                self.connection_stats[conn_id]["use_count"] += 1
            
            return True
        except Exception:
            return False
```

### 8.2 查询优化器
```python
class QueryOptimizer:
    """查询优化器"""
    
    def __init__(self):
        self.query_cache = {}
        self.execution_stats = {}
    
    def optimize_query(self, query: str, parameters: List[Any]) -> str:
        """优化查询"""
        
        # 查询缓存
        query_hash = hash(query + str(parameters))
        if query_hash in self.query_cache:
            return self.query_cache[query_hash]
        
        optimized_query = query
        
        # 添加适当的索引提示
        optimized_query = self._add_index_hints(optimized_query)
        
        # 优化JOIN顺序
        optimized_query = self._optimize_joins(optimized_query)
        
        # 添加查询计划缓存
        optimized_query = self._add_plan_cache(optimized_query)
        
        # 缓存优化结果
        self.query_cache[query_hash] = optimized_query
        
        return optimized_query
    
    def _add_index_hints(self, query: str) -> str:
        """添加索引提示"""
        # 基于查询模式添加索引提示
        if "WHERE workflow_id" in query:
            # 使用工作流ID索引
            pass
        
        if "WHERE status" in query:
            # 使用状态索引
            pass
        
        return query
    
    def record_execution_stats(self, query: str, execution_time: float):
        """记录执行统计"""
        query_pattern = self._extract_query_pattern(query)
        
        if query_pattern not in self.execution_stats:
            self.execution_stats[query_pattern] = {
                "total_executions": 0,
                "total_time": 0,
                "avg_time": 0,
                "max_time": 0,
                "min_time": float('inf')
            }
        
        stats = self.execution_stats[query_pattern]
        stats["total_executions"] += 1
        stats["total_time"] += execution_time
        stats["avg_time"] = stats["total_time"] / stats["total_executions"]
        stats["max_time"] = max(stats["max_time"], execution_time)
        stats["min_time"] = min(stats["min_time"], execution_time)
```

## 9. 监控和维护

### 9.1 性能监控器
```python
class PerformanceMonitor:
    """性能监控器"""
    
    def __init__(self):
        self.metrics = {
            "query_count": 0,
            "total_query_time": 0,
            "slow_queries": [],
            "connection_pool_stats": {},
            "cache_hit_rate": 0
        }
        self.slow_query_threshold = 1.0  # 1秒
    
    async def record_query_execution(self, 
                                   query: str,
                                   execution_time: float,
                                   result_count: int):
        """记录查询执行"""
        self.metrics["query_count"] += 1
        self.metrics["total_query_time"] += execution_time
        
        # 记录慢查询
        if execution_time > self.slow_query_threshold:
            self.metrics["slow_queries"].append({
                "query": query[:200] + "..." if len(query) > 200 else query,
                "execution_time": execution_time,
                "result_count": result_count,
                "timestamp": datetime.now()
            })
            
            # 只保留最近100个慢查询
            if len(self.metrics["slow_queries"]) > 100:
                self.metrics["slow_queries"] = self.metrics["slow_queries"][-100:]
    
    def get_performance_report(self) -> Dict[str, Any]:
        """获取性能报告"""
        avg_query_time = (
            self.metrics["total_query_time"] / self.metrics["query_count"]
            if self.metrics["query_count"] > 0 else 0
        )
        
        return {
            "total_queries": self.metrics["query_count"],
            "average_query_time": avg_query_time,
            "slow_query_count": len(self.metrics["slow_queries"]),
            "cache_hit_rate": self.metrics["cache_hit_rate"],
            "connection_pool_stats": self.metrics["connection_pool_stats"]
        }
```

## 10. 总结

数据持久化接口设计通过以下核心特性，为UAgent系统提供了可靠的数据存储支持：

1. **抽象接口设计**: 支持多种数据库后端的灵活切换
2. **多级缓存策略**: 内存+Redis的高性能缓存体系
3. **连接池优化**: 高效的数据库连接管理
4. **事务支持**: 完整的ACID事务保证
5. **性能监控**: 全面的性能指标收集和优化
6. **查询优化**: 智能的查询优化和执行计划缓存

该设计确保了UAgent系统的数据持久化需求得到高质量的满足，为系统的稳定运行提供了坚实的数据基础。

# UAgent 8段式上下文压缩算法

## 1. 概述

8段式上下文压缩算法是UAgent系统的核心记忆管理技术，借鉴Claude Code的优秀设计理念，通过智能化的信息压缩和优先级排序，在保持关键信息完整性的同时，有效控制上下文长度。

## 2. 设计理念

### 2.1 Claude Code 8段式结构借鉴
Claude Code的8段式压缩结构为：
1. **Primary Request and Intent**: 主要请求和意图
2. **Key Technical Concepts**: 关键技术概念
3. **Files and Code Sections**: 文件和代码段落
4. **Errors and fixes**: 错误和修复
5. **Problem Solving**: 问题解决
6. **All user messages**: 所有用户消息
7. **Pending Tasks**: 待处理任务
8. **Current Work**: 当前工作

### 2.2 UAgent适配性改进
针对多角色协作的特点，UAgent对8段式结构进行了适配：
- 增加角色协作信息的保留
- 优化交接内容的压缩策略
- 强化工作流状态的记录
- 提升技术细节的保真度

## 3. 压缩算法架构

```
┌─────────────────────────────────────────────────────────────┐
│              8-Segment Compression Engine                   │
├─────────────────────────────────────────────────────────────┤
│  Context Analyzer (上下文分析器)                            │
│  ├── Content Classifier (内容分类器)                       │
│  ├── Information Density Evaluator (信息密度评估器)        │
│  └── Relevance Scorer (相关性评分器)                       │
├─────────────────────────────────────────────────────────────┤
│  Information Prioritizer (信息优先级排序器)                 │
│  ├── Importance Ranker (重要性排序器)                       │
│  ├── Temporal Relevance Analyzer (时效性分析器)            │
│  └── Cross-Reference Detector (交叉引用检测器)              │
├─────────────────────────────────────────────────────────────┤
│  Compression Generator (压缩生成器)                         │
│  ├── Segment Compressor (段落压缩器)                       │
│  ├── Key Information Extractor (关键信息提取器)            │
│  └── Coherence Maintainer (连贯性维护器)                   │
├─────────────────────────────────────────────────────────────┤
│  Quality Validator (质量验证器)                             │
│  ├── Information Loss Detector (信息丢失检测器)            │
│  ├── Compression Ratio Controller (压缩比控制器)           │
│  └── Readability Checker (可读性检查器)                    │
└─────────────────────────────────────────────────────────────┘
```

## 4. 上下文分析器

### 4.1 内容分类器
```python
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from datetime import datetime
import re

@dataclass
class ContentClassification:
    """内容分类结果"""
    content_type: str
    confidence: float
    keywords: List[str]
    technical_level: str  # basic, intermediate, advanced
    actionability: str    # actionable, informational, historical

class ContentClassifier:
    """内容分类器"""
    
    def __init__(self):
        self.classification_patterns = {
            "technical_specification": [
                r"api|interface|schema|protocol",
                r"architecture|design|pattern",
                r"requirement|specification|standard"
            ],
            "code_implementation": [
                r"function|class|method|variable",
                r"import|export|module",
                r"implementation|code|syntax"
            ],
            "error_information": [
                r"error|exception|fail|bug",
                r"fix|solve|resolve|debug",
                r"issue|problem|trouble"
            ],
            "user_interaction": [
                r"user|question|ask|request",
                r"confirm|approve|feedback",
                r"input|output|response"
            ],
            "workflow_status": [
                r"task|workflow|progress",
                r"complete|pending|running",
                r"status|state|phase"
            ],
            "documentation": [
                r"document|readme|guide",
                r"instruction|tutorial|example",
                r"reference|manual|help"
            ]
        }
    
    async def classify_content(self, content: str, context: Dict[str, Any] = None) -> ContentClassification:
        """分类内容"""
        
        content_lower = content.lower()
        
        # 计算每种类型的匹配分数
        type_scores = {}
        for content_type, patterns in self.classification_patterns.items():
            score = 0
            matched_keywords = []
            
            for pattern in patterns:
                matches = re.findall(pattern, content_lower)
                if matches:
                    score += len(matches)
                    matched_keywords.extend(matches)
            
            if score > 0:
                type_scores[content_type] = {
                    "score": score,
                    "keywords": list(set(matched_keywords))
                }
        
        # 确定最可能的类型
        if type_scores:
            best_type = max(type_scores.keys(), key=lambda k: type_scores[k]["score"])
            confidence = min(type_scores[best_type]["score"] / 10.0, 1.0)
            keywords = type_scores[best_type]["keywords"]
        else:
            best_type = "general"
            confidence = 0.5
            keywords = []
        
        # 评估技术水平
        technical_level = self._assess_technical_level(content)
        
        # 评估可操作性
        actionability = self._assess_actionability(content)
        
        return ContentClassification(
            content_type=best_type,
            confidence=confidence,
            keywords=keywords,
            technical_level=technical_level,
            actionability=actionability
        )
    
    def _assess_technical_level(self, content: str) -> str:
        """评估技术水平"""
        technical_indicators = {
            "advanced": [
                r"algorithm|optimization|performance",
                r"architecture|scalability|distributed",
                r"security|encryption|authentication"
            ],
            "intermediate": [
                r"function|class|module|api",
                r"database|query|index",
                r"framework|library|dependency"
            ],
            "basic": [
                r"variable|string|number",
                r"file|folder|path",
                r"input|output|print"
            ]
        }
        
        content_lower = content.lower()
        level_scores = {}
        
        for level, patterns in technical_indicators.items():
            score = sum(len(re.findall(pattern, content_lower)) for pattern in patterns)
            level_scores[level] = score
        
        if level_scores["advanced"] > 0:
            return "advanced"
        elif level_scores["intermediate"] > 0:
            return "intermediate"
        else:
            return "basic"
    
    def _assess_actionability(self, content: str) -> str:
        """评估可操作性"""
        actionable_patterns = [
            r"need to|should|must|required",
            r"implement|create|build|develop",
            r"fix|update|modify|change",
            r"todo|task|action|next step"
        ]
        
        informational_patterns = [
            r"is|are|was|were",
            r"description|explanation|overview",
            r"example|sample|demo"
        ]
        
        content_lower = content.lower()
        
        actionable_score = sum(len(re.findall(pattern, content_lower)) for pattern in actionable_patterns)
        informational_score = sum(len(re.findall(pattern, content_lower)) for pattern in informational_patterns)
        
        if actionable_score > informational_score:
            return "actionable"
        elif informational_score > 0:
            return "informational"
        else:
            return "historical"
```

### 4.2 信息密度评估器
```python
class InformationDensityEvaluator:
    """信息密度评估器"""
    
    def __init__(self):
        self.high_density_indicators = [
            "technical terms frequency",
            "code snippet density",
            "specific numbers/metrics",
            "action items count",
            "decision points"
        ]
    
    async def evaluate_density(self, content: str, classification: ContentClassification) -> float:
        """评估信息密度"""
        
        # 基础密度评估
        base_density = self._calculate_base_density(content)
        
        # 根据内容类型调整
        type_multiplier = self._get_type_density_multiplier(classification.content_type)
        
        # 根据技术水平调整
        technical_multiplier = self._get_technical_density_multiplier(classification.technical_level)
        
        # 综合密度分数
        density_score = base_density * type_multiplier * technical_multiplier
        
        return min(density_score, 1.0)
    
    def _calculate_base_density(self, content: str) -> float:
        """计算基础信息密度"""
        if not content.strip():
            return 0.0
        
        words = content.split()
        sentences = re.split(r'[.!?]+', content)
        
        # 词汇多样性
        unique_words = len(set(word.lower() for word in words))
        vocabulary_diversity = unique_words / len(words) if words else 0
        
        # 句子复杂度
        avg_sentence_length = len(words) / len(sentences) if sentences else 0
        complexity_score = min(avg_sentence_length / 15.0, 1.0)
        
        # 技术术语密度
        technical_terms = len(re.findall(r'\b[A-Z][a-z]+[A-Z][a-z]*\b', content))  # CamelCase
        code_snippets = len(re.findall(r'`[^`]+`|```[^```]+```', content))
        technical_density = (technical_terms + code_snippets * 5) / len(words) if words else 0
        
        # 综合基础密度
        base_density = (vocabulary_diversity * 0.3 + 
                       complexity_score * 0.3 + 
                       technical_density * 0.4)
        
        return base_density
    
    def _get_type_density_multiplier(self, content_type: str) -> float:
        """根据内容类型获取密度倍数"""
        multipliers = {
            "technical_specification": 1.5,
            "code_implementation": 1.8,
            "error_information": 1.3,
            "user_interaction": 0.8,
            "workflow_status": 1.0,
            "documentation": 1.2,
            "general": 1.0
        }
        return multipliers.get(content_type, 1.0)
    
    def _get_technical_density_multiplier(self, technical_level: str) -> float:
        """根据技术水平获取密度倍数"""
        multipliers = {
            "advanced": 1.5,
            "intermediate": 1.2,
            "basic": 1.0
        }
        return multipliers.get(technical_level, 1.0)
```

### 4.3 相关性评分器
```python
class RelevanceScorer:
    """相关性评分器"""
    
    def __init__(self):
        self.current_context_weight = 0.4
        self.historical_context_weight = 0.3
        self.future_relevance_weight = 0.3
    
    async def score_relevance(self, 
                            content: str,
                            current_role: str,
                            workflow_stage: str,
                            context_history: List[str]) -> float:
        """评估内容相关性"""
        
        # 当前角色相关性
        current_relevance = await self._score_current_relevance(content, current_role)
        
        # 工作流阶段相关性
        stage_relevance = await self._score_stage_relevance(content, workflow_stage)
        
        # 历史上下文相关性
        historical_relevance = await self._score_historical_relevance(content, context_history)
        
        # 未来需求相关性
        future_relevance = await self._score_future_relevance(content, workflow_stage)
        
        # 综合相关性分数
        total_score = (
            current_relevance * self.current_context_weight +
            stage_relevance * 0.2 +
            historical_relevance * self.historical_context_weight +
            future_relevance * self.future_relevance_weight
        )
        
        return min(total_score, 1.0)
    
    async def _score_current_relevance(self, content: str, current_role: str) -> float:
        """评估当前角色相关性"""
        role_keywords = {
            "方案规划师": ["requirement", "architecture", "design", "plan", "specification"],
            "编码专家": ["code", "implement", "function", "class", "api", "development"],
            "测试工程师": ["test", "quality", "validation", "bug", "coverage"],
            "代码审查员": ["review", "quality", "standard", "security", "optimization"]
        }
        
        keywords = role_keywords.get(current_role, [])
        content_lower = content.lower()
        
        matches = sum(1 for keyword in keywords if keyword in content_lower)
        return min(matches / len(keywords), 1.0) if keywords else 0.5
    
    async def _score_stage_relevance(self, content: str, workflow_stage: str) -> float:
        """评估工作流阶段相关性"""
        stage_patterns = {
            "planning": [r"requirement|plan|design|architecture"],
            "implementation": [r"code|implement|develop|build"],
            "testing": [r"test|quality|validation|verify"],
            "review": [r"review|check|audit|optimize"]
        }
        
        patterns = stage_patterns.get(workflow_stage, [])
        content_lower = content.lower()
        
        matches = sum(len(re.findall(pattern, content_lower)) for pattern in patterns)
        return min(matches / 5.0, 1.0)  # 最多5个匹配视为满分
```

## 5. 信息优先级排序器

### 5.1 重要性排序器
```python
@dataclass
class ImportanceMetrics:
    """重要性指标"""
    criticality_score: float      # 关键性分数
    dependency_score: float       # 依赖性分数
    uniqueness_score: float       # 唯一性分数
    actionability_score: float    # 可操作性分数
    overall_importance: float     # 综合重要性

class ImportanceRanker:
    """重要性排序器"""
    
    def __init__(self):
        self.criticality_patterns = [
            r"critical|important|essential|required|must",
            r"error|fail|bug|issue|problem",
            r"security|vulnerability|risk",
            r"performance|optimization|bottleneck"
        ]
        
        self.dependency_indicators = [
            r"depend|require|need|prerequisite",
            r"before|after|then|next",
            r"input|output|parameter|result"
        ]
    
    async def rank_importance(self, 
                            content: str,
                            classification: ContentClassification,
                            context: Dict[str, Any]) -> ImportanceMetrics:
        """评估内容重要性"""
        
        # 关键性评估
        criticality_score = await self._assess_criticality(content, classification)
        
        # 依赖性评估
        dependency_score = await self._assess_dependency(content, context)
        
        # 唯一性评估
        uniqueness_score = await self._assess_uniqueness(content, context)
        
        # 可操作性评估
        actionability_score = await self._assess_actionability(content, classification)
        
        # 综合重要性计算
        overall_importance = (
            criticality_score * 0.35 +
            dependency_score * 0.25 +
            uniqueness_score * 0.20 +
            actionability_score * 0.20
        )
        
        return ImportanceMetrics(
            criticality_score=criticality_score,
            dependency_score=dependency_score,
            uniqueness_score=uniqueness_score,
            actionability_score=actionability_score,
            overall_importance=overall_importance
        )
    
    async def _assess_criticality(self, 
                                content: str, 
                                classification: ContentClassification) -> float:
        """评估关键性"""
        content_lower = content.lower()
        
        # 基于关键词的评估
        criticality_matches = sum(
            len(re.findall(pattern, content_lower)) 
            for pattern in self.criticality_patterns
        )
        
        keyword_score = min(criticality_matches / 3.0, 1.0)
        
        # 基于内容类型的调整
        type_multipliers = {
            "error_information": 1.5,
            "technical_specification": 1.3,
            "code_implementation": 1.2,
            "workflow_status": 1.1,
            "user_interaction": 0.9,
            "documentation": 0.8
        }
        
        type_multiplier = type_multipliers.get(classification.content_type, 1.0)
        
        return min(keyword_score * type_multiplier, 1.0)
    
    async def _assess_dependency(self, content: str, context: Dict[str, Any]) -> float:
        """评估依赖性"""
        content_lower = content.lower()
        
        # 依赖关键词匹配
        dependency_matches = sum(
            len(re.findall(pattern, content_lower)) 
            for pattern in self.dependency_indicators
        )
        
        # 检查是否被其他内容引用
        reference_count = context.get("reference_count", 0)
        
        # 检查是否引用其他内容
        outbound_references = len(re.findall(r"see|refer|mentioned|above|below", content_lower))
        
        dependency_score = (
            min(dependency_matches / 2.0, 0.4) +
            min(reference_count / 5.0, 0.4) +
            min(outbound_references / 3.0, 0.2)
        )
        
        return min(dependency_score, 1.0)
    
    async def _assess_uniqueness(self, content: str, context: Dict[str, Any]) -> float:
        """评估唯一性"""
        # 检查内容是否在其他地方重复
        similar_content_count = context.get("similar_content_count", 0)
        
        # 信息的独特性 - 基于技术术语和具体细节的密度
        technical_terms = len(re.findall(r'\b[A-Z][a-z]+[A-Z][a-z]*\b', content))
        specific_numbers = len(re.findall(r'\b\d+\b', content))
        code_blocks = len(re.findall(r'`[^`]+`|```[^```]+```', content))
        
        uniqueness_indicators = technical_terms + specific_numbers + code_blocks * 2
        
        uniqueness_score = (
            max(0, 1.0 - similar_content_count / 3.0) * 0.6 +
            min(uniqueness_indicators / 10.0, 1.0) * 0.4
        )
        
        return uniqueness_score
```

### 5.2 时效性分析器
```python
class TemporalRelevanceAnalyzer:
    """时效性分析器"""
    
    def __init__(self):
        self.temporal_decay_factor = 0.1  # 每天衰减10%
        self.future_relevance_boost = 1.2
    
    async def analyze_temporal_relevance(self, 
                                       content: str,
                                       timestamp: datetime,
                                       workflow_context: Dict[str, Any]) -> float:
        """分析时效性相关性"""
        
        current_time = datetime.now()
        time_diff = (current_time - timestamp).total_seconds()
        days_old = time_diff / (24 * 3600)
        
        # 时间衰减计算
        time_decay = max(0, 1.0 - days_old * self.temporal_decay_factor)
        
        # 内容时效性分类
        temporal_category = await self._categorize_temporal_relevance(content)
        
        # 根据类别调整时效性
        category_multipliers = {
            "immediate": 1.5,      # 立即相关
            "current_phase": 1.2,  # 当前阶段相关
            "future_relevant": 1.0, # 未来相关
            "historical": 0.7,     # 历史信息
            "timeless": 0.9        # 无时间性
        }
        
        category_multiplier = category_multipliers.get(temporal_category, 1.0)
        
        # 综合时效性分数
        temporal_score = time_decay * category_multiplier
        
        return min(temporal_score, 1.0)
    
    async def _categorize_temporal_relevance(self, content: str) -> str:
        """分类时效性相关性"""
        content_lower = content.lower()
        
        immediate_patterns = [
            r"now|current|today|urgent|asap",
            r"error|fail|broken|issue",
            r"need to|must do|required now"
        ]
        
        current_phase_patterns = [
            r"this phase|current stage|working on",
            r"implementing|developing|building",
            r"in progress|ongoing"
        ]
        
        future_patterns = [
            r"next|later|future|plan|will",
            r"after|then|subsequent",
            r"todo|pending|upcoming"
        ]
        
        historical_patterns = [
            r"was|were|had|did|completed",
            r"previous|before|earlier",
            r"history|past|old"
        ]
        
        # 检查各种模式
        if any(len(re.findall(pattern, content_lower)) > 0 for pattern in immediate_patterns):
            return "immediate"
        elif any(len(re.findall(pattern, content_lower)) > 0 for pattern in current_phase_patterns):
            return "current_phase"
        elif any(len(re.findall(pattern, content_lower)) > 0 for pattern in future_patterns):
            return "future_relevant"
        elif any(len(re.findall(pattern, content_lower)) > 0 for pattern in historical_patterns):
            return "historical"
        else:
            return "timeless"
```

## 6. 压缩生成器

### 6.1 段落压缩器
```python
class SegmentCompressor:
    """段落压缩器"""
    
    def __init__(self):
        self.compression_strategies = {
            "Primary Request and Intent": self._compress_primary_request,
            "Key Technical Concepts": self._compress_technical_concepts,
            "Files and Code Sections": self._compress_code_sections,
            "Errors and fixes": self._compress_errors_fixes,
            "Problem Solving": self._compress_problem_solving,
            "All user messages": self._compress_user_messages,
            "Pending Tasks": self._compress_pending_tasks,
            "Current Work": self._compress_current_work
        }
    
    async def compress_segment(self, 
                             segment_name: str,
                             content: str,
                             target_ratio: float,
                             importance_metrics: ImportanceMetrics) -> str:
        """压缩特定段落"""
        
        if not content.strip():
            return ""
        
        # 获取段落专用压缩策略
        compression_func = self.compression_strategies.get(
            segment_name, self._compress_generic
        )
        
        # 执行压缩
        compressed = await compression_func(content, target_ratio, importance_metrics)
        
        # 验证压缩质量
        if await self._validate_compression_quality(content, compressed, importance_metrics):
            return compressed
        else:
            # 压缩质量不达标，使用更保守的压缩
            return await self._conservative_compress(content, target_ratio * 1.2)
    
    async def _compress_primary_request(self, 
                                      content: str, 
                                      target_ratio: float,
                                      importance_metrics: ImportanceMetrics) -> str:
        """压缩主要请求和意图"""
        
        # 提取关键需求句子
        sentences = re.split(r'[.!?]+', content)
        
        # 识别需求关键词
        requirement_keywords = [
            "need", "want", "require", "should", "must",
            "implement", "create", "build", "develop"
        ]
        
        key_sentences = []
        for sentence in sentences:
            sentence = sentence.strip()
            if sentence and any(keyword in sentence.lower() for keyword in requirement_keywords):
                key_sentences.append(sentence)
        
        # 如果没有找到关键句子，保留前几句
        if not key_sentences:
            key_sentences = sentences[:2]
        
        # 组合关键信息
        compressed = ". ".join(key_sentences).strip()
        
        # 确保不超过目标长度
        target_length = int(len(content) * target_ratio)
        if len(compressed) > target_length:
            compressed = compressed[:target_length] + "..."
        
        return compressed
    
    async def _compress_technical_concepts(self, 
                                         content: str, 
                                         target_ratio: float,
                                         importance_metrics: ImportanceMetrics) -> str:
        """压缩技术概念"""
        
        # 提取技术术语和关键概念
        technical_terms = re.findall(r'\b[A-Z][a-z]+[A-Z][a-z]*\b', content)  # CamelCase
        acronyms = re.findall(r'\b[A-Z]{2,}\b', content)  # 缩写
        
        # 提取关键技术句子
        tech_keywords = [
            "architecture", "design", "pattern", "framework",
            "database", "api", "service", "component",
            "algorithm", "protocol", "schema", "interface"
        ]
        
        sentences = re.split(r'[.!?]+', content)
        tech_sentences = []
        
        for sentence in sentences:
            sentence = sentence.strip()
            if sentence:
                # 检查是否包含技术关键词或术语
                has_tech_keyword = any(keyword in sentence.lower() for keyword in tech_keywords)
                has_tech_term = any(term in sentence for term in technical_terms + acronyms)
                
                if has_tech_keyword or has_tech_term:
                    tech_sentences.append(sentence)
        
        # 保留重要的技术术语列表
        important_terms = list(set(technical_terms + acronyms))
        if important_terms:
            terms_summary = f"Key terms: {', '.join(important_terms[:10])}"
            tech_sentences.insert(0, terms_summary)
        
        # 组合压缩结果
        compressed = ". ".join(tech_sentences)
        
        # 长度控制
        target_length = int(len(content) * target_ratio)
        if len(compressed) > target_length:
            # 优先保留术语总结和前几个技术句子
            if important_terms:
                compressed = terms_summary
                remaining_length = target_length - len(terms_summary) - 3
                if remaining_length > 0 and tech_sentences[1:]:
                    additional_content = ". ".join(tech_sentences[1:])[:remaining_length]
                    compressed += ". " + additional_content + "..."
            else:
                compressed = compressed[:target_length] + "..."
        
        return compressed
    
    async def _compress_code_sections(self, 
                                    content: str, 
                                    target_ratio: float,
                                    importance_metrics: ImportanceMetrics) -> str:
        """压缩代码段落"""
        
        # 提取代码块
        code_blocks = re.findall(r'```[^```]+```', content)
        inline_code = re.findall(r'`[^`]+`', content)
        
        # 提取文件引用
        file_references = re.findall(r'\b\w+\.\w+\b', content)
        
        compressed_parts = []
        
        # 处理代码块
        if code_blocks:
            # 只保留最重要的代码块（通常是最长的或包含关键函数的）
            important_blocks = sorted(code_blocks, key=len, reverse=True)[:2]
            compressed_parts.extend(important_blocks)
        
        # 处理内联代码
        if inline_code:
            # 保留独特的内联代码
            unique_inline = list(set(inline_code))[:5]
            if unique_inline:
                compressed_parts.append(f"Key code elements: {', '.join(unique_inline)}")
        
        # 处理文件引用
        if file_references:
            unique_files = list(set(file_references))[:8]
            if unique_files:
                compressed_parts.append(f"Referenced files: {', '.join(unique_files)}")
        
        # 提取非代码的重要描述
        text_content = content
        for block in code_blocks:
            text_content = text_content.replace(block, "")
        for inline in inline_code:
            text_content = text_content.replace(inline, "")
        
        # 从文本中提取关键句子
        sentences = re.split(r'[.!?]+', text_content)
        key_sentences = [s.strip() for s in sentences if s.strip() and len(s.strip()) > 10][:3]
        
        compressed_parts.extend(key_sentences)
        
        # 组合结果
        compressed = "\n\n".join(compressed_parts)
        
        # 长度控制
        target_length = int(len(content) * target_ratio)
        if len(compressed) > target_length:
            compressed = compressed[:target_length] + "..."
        
        return compressed
```

### 6.2 关键信息提取器
```python
class KeyInformationExtractor:
    """关键信息提取器"""
    
    def __init__(self):
        self.extraction_prompt_template = """
从以下内容中提取最关键的信息，保持原意不变的前提下进行压缩。

## 原始内容
{original_content}

## 压缩要求
- 目标压缩比例: {target_ratio}
- 内容重要性: {importance_level}
- 内容类型: {content_type}

## 提取原则
1. 保留所有关键技术细节和决策
2. 移除重复和冗余信息
3. 保持逻辑连贯性
4. 优先保留可操作的信息
5. 保留重要的上下文关联

请提取关键信息：
        """
    
    async def extract_key_information(self, 
                                    content: str,
                                    target_ratio: float,
                                    classification: ContentClassification,
                                    importance_metrics: ImportanceMetrics) -> str:
        """提取关键信息"""
        
        # 如果内容很短，直接返回
        if len(content) <= 100:
            return content
        
        # 基于规则的快速提取
        rule_based_result = await self._rule_based_extraction(
            content, target_ratio, classification
        )
        
        # 如果重要性很高，使用LLM进行精确提取
        if importance_metrics.overall_importance > 0.8:
            llm_result = await self._llm_based_extraction(
                content, target_ratio, classification, importance_metrics
            )
            
            # 选择更好的结果
            if len(llm_result) > 0 and self._evaluate_extraction_quality(content, llm_result) > 0.7:
                return llm_result
        
        return rule_based_result
    
    async def _rule_based_extraction(self, 
                                   content: str,
                                   target_ratio: float,
                                   classification: ContentClassification) -> str:
        """基于规则的信息提取"""
        
        sentences = re.split(r'[.!?]+', content)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        # 根据内容类型确定提取策略
        if classification.content_type == "technical_specification":
            return await self._extract_technical_specs(sentences, target_ratio)
        elif classification.content_type == "code_implementation":
            return await self._extract_code_info(sentences, target_ratio)
        elif classification.content_type == "error_information":
            return await self._extract_error_info(sentences, target_ratio)
        else:
            return await self._extract_generic_info(sentences, target_ratio)
    
    async def _extract_technical_specs(self, sentences: List[str], target_ratio: float) -> str:
        """提取技术规范信息"""
        
        # 技术规范关键词
        spec_keywords = [
            "requirement", "specification", "interface", "api",
            "architecture", "design", "protocol", "schema",
            "must", "should", "shall", "required"
        ]
        
        # 评分和排序句子
        scored_sentences = []
        for sentence in sentences:
            score = 0
            sentence_lower = sentence.lower()
            
            # 关键词匹配分数
            for keyword in spec_keywords:
                if keyword in sentence_lower:
                    score += 1
            
            # 技术术语分数
            tech_terms = len(re.findall(r'\b[A-Z][a-z]+[A-Z][a-z]*\b', sentence))
            score += tech_terms * 0.5
            
            # 数字和具体值分数
            numbers = len(re.findall(r'\b\d+\b', sentence))
            score += numbers * 0.3
            
            if score > 0:
                scored_sentences.append((sentence, score))
        
        # 按分数排序并选择顶部句子
        scored_sentences.sort(key=lambda x: x[1], reverse=True)
        
        target_count = max(1, int(len(sentences) * target_ratio))
        selected_sentences = [s[0] for s in scored_sentences[:target_count]]
        
        return ". ".join(selected_sentences)
    
    async def _llm_based_extraction(self, 
                                  content: str,
                                  target_ratio: float,
                                  classification: ContentClassification,
                                  importance_metrics: ImportanceMetrics) -> str:
        """基于LLM的信息提取"""
        
        prompt = self.extraction_prompt_template.format(
            original_content=content,
            target_ratio=target_ratio,
            importance_level=importance_metrics.overall_importance,
            content_type=classification.content_type
        )
        
        try:
            # 调用LLM进行提取
            response = await self.llm_client.generate(prompt)
            
            # 验证提取结果
            if self._validate_extraction_result(content, response):
                return response.strip()
            else:
                # LLM提取失败，回退到规则方法
                return await self._rule_based_extraction(content, target_ratio, classification)
                
        except Exception as e:
            logger.warning(f"LLM extraction failed: {e}")
            return await self._rule_based_extraction(content, target_ratio, classification)
```

## 7. 质量验证器

### 7.1 信息丢失检测器
```python
class InformationLossDetector:
    """信息丢失检测器"""
    
    def __init__(self):
        self.critical_loss_threshold = 0.3  # 关键信息丢失阈值
        self.acceptable_loss_threshold = 0.6  # 可接受信息丢失阈值
    
    async def detect_information_loss(self, 
                                    original: str,
                                    compressed: str,
                                    importance_metrics: ImportanceMetrics) -> Dict[str, Any]:
        """检测信息丢失"""
        
        # 基础指标计算
        length_ratio = len(compressed) / len(original) if original else 0
        
        # 关键词保留率
        keyword_retention = await self._calculate_keyword_retention(original, compressed)
        
        # 技术术语保留率
        technical_retention = await self._calculate_technical_retention(original, compressed)
        
        # 数值信息保留率
        numerical_retention = await self._calculate_numerical_retention(original, compressed)
        
        # 结构完整性
        structural_integrity = await self._assess_structural_integrity(original, compressed)
        
        # 语义连贯性
        semantic_coherence = await self._assess_semantic_coherence(compressed)
        
        # 综合信息保留率
        overall_retention = (
            keyword_retention * 0.25 +
            technical_retention * 0.30 +
            numerical_retention * 0.15 +
            structural_integrity * 0.15 +
            semantic_coherence * 0.15
        )
        
        # 判断信息丢失程度
        loss_level = self._categorize_loss_level(overall_retention, importance_metrics)
        
        return {
            "length_ratio": length_ratio,
            "keyword_retention": keyword_retention,
            "technical_retention": technical_retention,
            "numerical_retention": numerical_retention,
            "structural_integrity": structural_integrity,
            "semantic_coherence": semantic_coherence,
            "overall_retention": overall_retention,
            "loss_level": loss_level,
            "is_acceptable": loss_level in ["minimal", "acceptable"]
        }
    
    async def _calculate_keyword_retention(self, original: str, compressed: str) -> float:
        """计算关键词保留率"""
        
        # 提取重要关键词
        important_keywords = [
            "must", "should", "required", "critical", "important",
            "error", "fail", "success", "complete",
            "api", "interface", "service", "component",
            "database", "query", "index", "schema"
        ]
        
        original_lower = original.lower()
        compressed_lower = compressed.lower()
        
        original_keywords = [kw for kw in important_keywords if kw in original_lower]
        retained_keywords = [kw for kw in original_keywords if kw in compressed_lower]
        
        if not original_keywords:
            return 1.0  # 如果原文没有关键词，认为完全保留
        
        return len(retained_keywords) / len(original_keywords)
    
    async def _calculate_technical_retention(self, original: str, compressed: str) -> float:
        """计算技术术语保留率"""
        
        # 提取技术术语
        original_terms = set(re.findall(r'\b[A-Z][a-z]+[A-Z][a-z]*\b', original))  # CamelCase
        original_terms.update(re.findall(r'\b[A-Z]{2,}\b', original))  # 缩写
        
        compressed_terms = set(re.findall(r'\b[A-Z][a-z]+[A-Z][a-z]*\b', compressed))
        compressed_terms.update(re.findall(r'\b[A-Z]{2,}\b', compressed))
        
        if not original_terms:
            return 1.0
        
        retained_terms = original_terms.intersection(compressed_terms)
        return len(retained_terms) / len(original_terms)
    
    async def _calculate_numerical_retention(self, original: str, compressed: str) -> float:
        """计算数值信息保留率"""
        
        original_numbers = set(re.findall(r'\b\d+(?:\.\d+)?\b', original))
        compressed_numbers = set(re.findall(r'\b\d+(?:\.\d+)?\b', compressed))
        
        if not original_numbers:
            return 1.0
        
        retained_numbers = original_numbers.intersection(compressed_numbers)
        return len(retained_numbers) / len(original_numbers)
    
    def _categorize_loss_level(self, retention_rate: float, importance_metrics: ImportanceMetrics) -> str:
        """分类信息丢失程度"""
        
        # 根据重要性调整阈值
        if importance_metrics.overall_importance > 0.8:
            # 高重要性内容，要求更高的保留率
            if retention_rate >= 0.8:
                return "minimal"
            elif retention_rate >= 0.6:
                return "acceptable"
            elif retention_rate >= 0.4:
                return "moderate"
            else:
                return "severe"
        else:
            # 普通重要性内容
            if retention_rate >= 0.7:
                return "minimal"
            elif retention_rate >= 0.5:
                return "acceptable"
            elif retention_rate >= 0.3:
                return "moderate"
            else:
                return "severe"
```

### 7.2 压缩比控制器
```python
class CompressionRatioController:
    """压缩比控制器"""
    
    def __init__(self):
        self.min_compression_ratio = 0.3  # 最小压缩比
        self.max_compression_ratio = 0.9  # 最大压缩比
        self.adaptive_adjustment_factor = 0.1
    
    async def control_compression_ratio(self, 
                                      original_content: str,
                                      compressed_content: str,
                                      target_ratio: float,
                                      importance_metrics: ImportanceMetrics,
                                      loss_detection_result: Dict[str, Any]) -> str:
        """控制压缩比例"""
        
        current_ratio = len(compressed_content) / len(original_content) if original_content else 0
        
        # 检查是否需要调整
        adjustment_needed = await self._assess_adjustment_need(
            current_ratio, target_ratio, importance_metrics, loss_detection_result
        )
        
        if not adjustment_needed:
            return compressed_content
        
        # 计算新的目标比例
        new_target_ratio = await self._calculate_new_target_ratio(
            current_ratio, target_ratio, importance_metrics, loss_detection_result
        )
        
        # 重新压缩
        if new_target_ratio > current_ratio:
            # 需要保留更多内容
            return await self._expand_compression(
                original_content, compressed_content, new_target_ratio
            )
        else:
            # 需要进一步压缩
            return await self._enhance_compression(
                original_content, compressed_content, new_target_ratio
            )
    
    async def _assess_adjustment_need(self, 
                                    current_ratio: float,
                                    target_ratio: float,
                                    importance_metrics: ImportanceMetrics,
                                    loss_detection_result: Dict[str, Any]) -> bool:
        """评估是否需要调整"""
        
        # 检查信息丢失是否过多
        if loss_detection_result["loss_level"] == "severe":
            return True
        
        # 检查压缩比是否偏离目标太多
        ratio_deviation = abs(current_ratio - target_ratio)
        if ratio_deviation > 0.2:
            return True
        
        # 检查重要内容的保留情况
        if (importance_metrics.overall_importance > 0.8 and 
            loss_detection_result["overall_retention"] < 0.7):
            return True
        
        return False
    
    async def _calculate_new_target_ratio(self, 
                                        current_ratio: float,
                                        target_ratio: float,
                                        importance_metrics: ImportanceMetrics,
                                        loss_detection_result: Dict[str, Any]) -> float:
        """计算新的目标压缩比"""
        
        adjustment = 0
        
        # 基于信息丢失程度调整
        loss_level = loss_detection_result["loss_level"]
        if loss_level == "severe":
            adjustment += 0.3
        elif loss_level == "moderate":
            adjustment += 0.15
        
        # 基于重要性调整
        if importance_metrics.overall_importance > 0.8:
            adjustment += 0.1
        
        # 基于保留率调整
        retention_rate = loss_detection_result["overall_retention"]
        if retention_rate < 0.5:
            adjustment += 0.2
        elif retention_rate < 0.7:
            adjustment += 0.1
        
        new_ratio = min(current_ratio + adjustment, self.max_compression_ratio)
        new_ratio = max(new_ratio, self.min_compression_ratio)
        
        return new_ratio
```

## 8. 完整压缩流程

### 8.1 主压缩引擎
```python
class EightSegmentCompressionEngine:
    """8段式压缩引擎"""
    
    def __init__(self):
        self.context_analyzer = ContextAnalyzer()
        self.information_prioritizer = InformationPrioritizer()
        self.compression_generator = CompressionGenerator()
        self.quality_validator = QualityValidator()
    
    async def compress_context(self, 
                             context: IsolatedRoleContext,
                             target_compression_ratio: float = 0.6,
                             role_context: Dict[str, Any] = None) -> str:
        """执行完整的8段式压缩"""
        
        # 1. 分析上下文
        context_analysis = await self.context_analyzer.analyze_context(context)
        
        # 2. 信息优先级排序
        prioritization_result = await self.information_prioritizer.prioritize_information(
            context, context_analysis, role_context
        )
        
        # 3. 生成压缩内容
        compression_result = await self.compression_generator.generate_compressed_context(
            context, prioritization_result, target_compression_ratio
        )
        
        # 4. 质量验证
        quality_result = await self.quality_validator.validate_compression_quality(
            context, compression_result, prioritization_result
        )
        
        # 5. 质量调整（如需要）
        if not quality_result.is_acceptable:
            compression_result = await self._adjust_compression_quality(
                context, compression_result, quality_result
            )
        
        # 6. 生成最终压缩结果
        final_compressed = await self._format_final_compression(
            compression_result, context.role
        )
        
        return final_compressed
    
    async def _format_final_compression(self, 
                                      compression_result: CompressionResult,
                                      role: str) -> str:
        """格式化最终压缩结果"""
        
        sections = []
        
        for segment_name in [
            "Primary Request and Intent",
            "Key Technical Concepts", 
            "Files and Code Sections",
            "Errors and fixes",
            "Problem Solving",
            "All user messages",
            "Pending Tasks",
            "Current Work"
        ]:
            compressed_content = compression_result.segments.get(segment_name, "")
            if compressed_content.strip():
                sections.append(f"## {segment_name}\n{compressed_content}")
        
        # 添加压缩元信息
        compression_meta = f"""
## Compression Metadata
- Original length: {compression_result.original_length} chars
- Compressed length: {compression_result.compressed_length} chars  
- Compression ratio: {compression_result.actual_ratio:.2f}
- Quality score: {compression_result.quality_score:.2f}
- Role: {role}
        """
        
        sections.append(compression_meta.strip())
        
        return "\n\n".join(sections)
```

## 9. 性能优化和监控

### 9.1 压缩性能监控
```python
class CompressionPerformanceMonitor:
    """压缩性能监控"""
    
    def __init__(self):
        self.compression_history = []
        self.performance_metrics = {}
    
    async def monitor_compression_performance(self, 
                                           compression_session: CompressionSession):
        """监控压缩性能"""
        
        # 记录压缩会话
        self.compression_history.append(compression_session)
        
        # 更新性能指标
        await self._update_performance_metrics()
        
        # 检查性能异常
        anomalies = await self._detect_performance_anomalies()
        
        if anomalies:
            await self._handle_performance_issues(anomalies)
    
    async def _update_performance_metrics(self):
        """更新性能指标"""
        recent_sessions = self.compression_history[-100:]  # 最近100次压缩
        
        if not recent_sessions:
            return
        
        # 平均压缩时间
        avg_compression_time = sum(s.execution_time for s in recent_sessions) / len(recent_sessions)
        
        # 平均压缩比
        avg_compression_ratio = sum(s.compression_ratio for s in recent_sessions) / len(recent_sessions)
        
        # 平均质量分数
        avg_quality_score = sum(s.quality_score for s in recent_sessions) / len(recent_sessions)
        
        # 成功率
        success_rate = sum(1 for s in recent_sessions if s.success) / len(recent_sessions)
        
        self.performance_metrics.update({
            "avg_compression_time": avg_compression_time,
            "avg_compression_ratio": avg_compression_ratio,
            "avg_quality_score": avg_quality_score,
            "success_rate": success_rate,
            "total_compressions": len(self.compression_history)
        })
```

## 10. 总结

8段式上下文压缩算法通过以下核心技术，为UAgent系统提供了高质量的记忆管理：

1. **智能内容分析**: 深度理解内容类型、重要性和相关性
2. **科学优先级排序**: 基于多维度指标的信息重要性评估
3. **精准压缩生成**: 针对不同段落类型的专业化压缩策略
4. **严格质量控制**: 全面的信息丢失检测和质量验证
5. **自适应优化**: 根据压缩效果动态调整策略

该算法确保在长对话过程中保持关键信息的完整性，同时有效控制上下文长度，为UAgent系统的持续高质量运行提供了坚实的技术保障。
